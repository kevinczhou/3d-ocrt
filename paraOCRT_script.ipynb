{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id = 'zebrafish'  # should be 'fruitfly', 'zebrafish', 'trachea', or 'esophagus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "import scipy.io\n",
    "from paraOCRT import *\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "batch_size_stratified = 50  # this times the number of angles is the total number of Ascans per batch\n",
    "n_batches = 400**2 / batch_size_stratified\n",
    "scale = .5\n",
    "RI_scale = .25\n",
    "pixel_size = 1  # recon pizel size in um, when scale=1\n",
    "momentum = 4 / n_batches  # see paper for explanation\n",
    "lr_multiplier = .1  # rescale all learning rates by this\n",
    "th_range = np.arange(96); th_range = np.delete(th_range, [27, 72, 73, 74, 75])  # batch from a subset of the angles\n",
    "update_gradient_after = 1000  # only start updating tf.Variables after this many iterations\n",
    "loss_print_iter = 10  # print loss every this many iters\n",
    "plot_iter = 1000  # plot every this many iters\n",
    "num_iter = 50000  # total number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sample_id == 'trachea':\n",
    "    path = './data/trachea/trachea.hdf5'  # where the dataset is stored\n",
    "    restore_path = './data/trachea/tf_ckpts/'  # initialize with parameters optimized on calibration sample\n",
    "    recon_shape = (1500, 1500, 500)\n",
    "    xyz_offset = np.array([0, 190, 0])  # to center the reconstruction\n",
    "    \n",
    "elif sample_id == 'esophagus':\n",
    "    path = './data/esophagus/esophagus.hdf5'\n",
    "    restore_path = './data/esophagus/tf_ckpts/'\n",
    "    recon_shape = (1000, 1000, 850)\n",
    "    xyz_offset = np.array([0, 330, 0]) \n",
    "    \n",
    "elif sample_id == 'fruitfly':\n",
    "    path = './data/fruitfly/fruitfly.hdf5'\n",
    "    restore_path = './data/fruitfly/tf_ckpts/'\n",
    "    recon_shape = (1200, 1200, 1000)\n",
    "    xyz_offset = np.array([0, 280, 80])\n",
    "\n",
    "elif sample_id == 'zebrafish':\n",
    "    path = './data/zebrafish/zebrafish.hdf5'\n",
    "    restore_path = './data/zebrafish/tf_ckpts/'\n",
    "    recon_shape = (1200, 1200, 700)\n",
    "    xyz_offset = np.array([0, 130, 20])\n",
    "    \n",
    "else:\n",
    "    raise Exception('invalid sample_id: ' + sample_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(path, 'r') as f:\n",
    "    num_x = f.attrs['num_x']\n",
    "    num_y = f.attrs['num_y']\n",
    "    num_th = f.attrs['num_th']\n",
    "    A_scan_num = f.attrs['Ascan_len']\n",
    "    stack_shape = (num_th, num_x, num_y, A_scan_num)\n",
    "    num_Ascans = np.prod(stack_shape[:-1])  # number of A-scans total\n",
    "    hx = np.array(f['galvo_x'])\n",
    "    hy = np.array(f['galvo_y'])\n",
    "    x = np.array(f['probe_x'])\n",
    "    y = np.array(f['probe_y'])\n",
    "y -= 10\n",
    "hx = - hx / hx[0] # normalize to first value\n",
    "hy = - hy / hy[0]\n",
    "galvo_xy = np.stack([hx, hy], axis=1)\n",
    "probe_xy = np.stack([x, y], axis=1)\n",
    "    \n",
    "# instantiate paraOCRT:\n",
    "a = paraOCRT(recon_shape=recon_shape, RI_shape_scale=RI_scale, dxyz=pixel_size, hdf5_path=path, \n",
    "             batch_size=batch_size_stratified, scale=scale, momentum=momentum,\n",
    "            )\n",
    "a.shuffle_size = 1  # don't shuffle\n",
    "a.prefetch = 5\n",
    "a.th_range = th_range\n",
    "a.data_num_x = num_x\n",
    "a.data_num_y = num_y\n",
    "a.data_num_z = A_scan_num\n",
    "a.z_downsamp = 10  # downsample the A-scan when propagating to save memory/time\n",
    "a.use_first_reflection_RI_loss = True\n",
    "a.correct_momentum_bias_in_loss = True\n",
    "a.xyz_offset = xyz_offset\n",
    "a.n_back = 1.342\n",
    "a.z_start = 0\n",
    "a.z_end = 1500\n",
    "    \n",
    "learning_rates = {'f_mirror': -1e-1, 'f_lens': -1e-1,  # negative means not optimized\n",
    "                  'galvo_xy': 1e-3, 'galvo_normal': 1e-3, 'galvo_theta': 1e-3,\n",
    "                  'probe_dx': 1e-2, 'probe_dy': 1e-2, 'probe_z': 1e-2, 'probe_normal': 1e-3,\n",
    "                  'probe_theta': 1e-3, 'd_before_f': 1e-3, 'RI': 1e-2, 'Ascan_background': 1e-3,\n",
    "                  'delta_r': 1e-3, 'delta_u': 1e-3, 'galvo_xy_per': 1e-3, \n",
    "                  'galvo_theta_in_plane_per': 1e-3, 'probe_dxyz_per': 1e-3,\n",
    "                  'r_2nd_order': 1e-3, 'u_2nd_order': 1e-3,\n",
    "                  'dome_inner_radius': -1e-3, 'dome_outer_radius': -1e-3, 'dome_center': 1e-3,\n",
    "                  'r_higher_order': 1e-3, 'u_higher_order': 1e-3,\n",
    "                 }\n",
    "# only optimize RI:\n",
    "for key in learning_rates:\n",
    "    if key == 'RI' or key == 'Ascan_background':\n",
    "        pass\n",
    "    else:\n",
    "        learning_rates[key] = -1\n",
    "# ... and boundary conditions:\n",
    "learning_rates['delta_r'] = 1\n",
    "learning_rates['delta_u'] = 1e-2\n",
    "learning_rates['galvo_theta_in_plane_per'] = 1e-3\n",
    "    \n",
    "if lr_multiplier is not None:\n",
    "    for key in learning_rates:\n",
    "        learning_rates[key] *= lr_multiplier\n",
    "        \n",
    "variable_initial_values = {'f_mirror': 12.5, 'f_lens': 30, 'galvo_xy': .7 * np.ones(2, dtype=np.float32) *12/7,  \n",
    "                           'galvo_normal': np.array((1e-7, 1e-7, -1), dtype=np.float32),  \n",
    "                           'galvo_theta': 0, 'probe_dx': 0, 'probe_dy': 0,  'probe_z': -10,  \n",
    "                           'probe_normal': np.array((1e-7, 1e-7, -1), dtype=np.float32),\n",
    "                           'probe_theta': 0, 'd_before_f': .725,\n",
    "                           'effective_inverse_momentum': 11446\n",
    "                          }  \n",
    "\n",
    "a.create_variables(nominal_probe_xy=probe_xy, nominal_galvo_xy=galvo_xy, \n",
    "                   propagation_model='parabolic_dome_nonparametric_higher_order_correction',\n",
    "                   learning_rates=learning_rates, variable_initial_values=variable_initial_values)\n",
    "reg_coefs = {'first_reflection_RI': 1e-5, 'RI_TV2': 3e-8}  # regularization coefficients\n",
    "    \n",
    "dataset = a.generate_dataset()  # dataset yields stratified batches of random A-scans\n",
    "\n",
    "losses = list()\n",
    "ii = 0  # optimization iteration counter\n",
    "\n",
    "# restoring parameters from a checkpoint:\n",
    "if restore_path is not None:\n",
    "    a.checkpoint_all_variables(path=restore_path, skip_saving=True, var_ignore=['RI'])\n",
    "    a.restore_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# optimization loop:\n",
    "for Ascan_batch, batch_inds in dataset:\n",
    "    if ii == 0:\n",
    "        # use the average of the first batch of A-scans as the initial guess\n",
    "        Ascan_background_init = Ascan_batch.numpy().mean(0).mean(0)\n",
    "        a.train_var_dict['Ascan_background'].assign(Ascan_background_init)\n",
    "  \n",
    "    start = time()\n",
    "    \n",
    "    if ii > update_gradient_after:\n",
    "        update_gradient = True\n",
    "    else:\n",
    "        update_gradient = False\n",
    "    \n",
    "    try:  # update variables\n",
    "        loss_i, recon_i = a.gradient_update(Ascan_batch, batch_inds, update_gradient=update_gradient,\n",
    "                                            reg_coefs=reg_coefs)\n",
    "    except ValueError:\n",
    "        # for some reason, I get an error when update_gradient changes from False to True, but only the first time\n",
    "        pass\n",
    "    \n",
    "    if type(loss_i) is list:\n",
    "        losses.append([loss.numpy() for loss in loss_i])\n",
    "    else:\n",
    "        losses.append(loss_i.numpy())\n",
    "        \n",
    "    # force RI to be greater than that of background:\n",
    "    a.train_var_dict['RI'].assign(tf.math.maximum(a.train_var_dict['RI'], a.n_back))\n",
    "        \n",
    "    if ii % loss_print_iter == 0:\n",
    "        print(ii, losses[-1], time()-start)\n",
    "    \n",
    "    if ii % plot_iter == 0:\n",
    "        summarize_recon(recon_i.numpy())  # plot cross sections of the reconstruction;\n",
    "        summarize_recon(a.train_var_dict['RI'].numpy(), 'viridis', True)\n",
    "\n",
    "        plt.plot(losses)\n",
    "        plt.title('loss history')\n",
    "        plt.show()\n",
    "        plt.plot(np.log(losses))\n",
    "        plt.title('log loss history')\n",
    "        plt.show()\n",
    "        if ii > 500:\n",
    "            plt.plot(np.convolve(np.array([loss[0] for loss in losses][50:]), np.ones(2000)/2000, 'valid'))\n",
    "            plt.title('blurred loss history')\n",
    "            plt.show()\n",
    "\n",
    "    if ii == num_iter:\n",
    "        break\n",
    "    else:\n",
    "        ii += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results:\n",
    "scipy.io.savemat(sample_id + '.mat', {'recon': recon_i.numpy(),\n",
    "                                      'RI': a.train_var_dict['RI'].numpy(),\n",
    "                                      'losses': losses\n",
    "                                     })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_gcp",
   "language": "python",
   "name": "tf2_gcp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
